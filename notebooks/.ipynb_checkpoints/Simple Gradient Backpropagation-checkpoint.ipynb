{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gradient Calculation checked with pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  2.  3. -4.]]\n",
      "[[ 2.  -3. ]\n",
      " [ 0.5  3. ]\n",
      " [ 3.  -1. ]\n",
      " [ 3.   0. ]]\n",
      "[[1.]\n",
      " [1.]]\n",
      "sigmoid(0): 0.5\n",
      "hidden in [[0. 0.]]\n",
      "nn(x) [[1.]]\n",
      "hidden in [[0. 0.]]\n",
      "loss((x,y)) [[1.]]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0, -4.0]).reshape(1,4)\n",
    "y = np.array([0.0]).reshape(1, 1)\n",
    "print(x)\n",
    "W1 = np.array([[2.0, 0.5, 3.0, 3.0], [-3.0, 3.0, -1.0, 0.0]]).T\n",
    "print(W1)\n",
    "W2 = np.array([1.0, 1.0]).reshape(2, 1)\n",
    "print(W2)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(x))\n",
    "\n",
    "def nn(x, W1, W2):\n",
    "    print('hidden in', x @ W1)\n",
    "    return sigmoid(x @ W1)@ W2\n",
    "\n",
    "def loss(x, y, W1, W2):\n",
    "    pred = nn(x, W1, W2)\n",
    "    diff = pred - y\n",
    "    loss = diff**2\n",
    "    return loss\n",
    "print('sigmoid(0):', sigmoid(0))\n",
    "print('nn(x)', nn(x, W1, W2))\n",
    "print('loss((x,y))', loss(x, y, W1, W2))\n",
    "\n",
    "dl_dnnx = 2 \n",
    "dnnx_dw2 = \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of f(z)=1/(1+e^{-z}) at x = 0 tensor([ 0.2500])\n",
      "loss(x, y) tensor([[ 1.]], dtype=torch.float64)\n",
      "nn(x):  tensor([[ 1.]], dtype=torch.float64)\n",
      "Results\n",
      "L/TW2 grad\n",
      " tensor([[ 1.],\n",
      "        [ 1.]], dtype=torch.float64)\n",
      "L/TW1 grad\n",
      " tensor([[-0.5000, -0.5000],\n",
      "        [-1.0000, -1.0000],\n",
      "        [-1.5000, -1.5000],\n",
      "        [ 2.0000,  2.0000]], dtype=torch.float64)\n",
      "Lets do the steps\n",
      "L/nnx grad\n",
      " tensor([[ 2.]], dtype=torch.float64)\n",
      "L/hidden_out grad\n",
      " tensor([[ 2.,  2.]], dtype=torch.float64)\n",
      "L/hidden_in grad\n",
      " tensor([[-0.5000, -0.5000]], dtype=torch.float64)\n",
      "Lets do in steps we know the result\n",
      "nnx/TW2 grad\n",
      " tensor([[ 1.],\n",
      "        [ 1.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentation\n",
    "import torch\n",
    "z = torch.zeros(1, requires_grad=True)\n",
    "sz = 1.0 / (1 + torch.exp(-z))\n",
    "sz.backward()\n",
    "print('gradient of f(z)=1/(1+e^{-z}) at x = 0', z.grad)\n",
    "\n",
    "def tsigmoid(z):    \n",
    "    return 1.0/(1.0 + torch.exp(z))\n",
    "\n",
    "TW1 = torch.from_numpy(W1)\n",
    "TW1.requires_grad=True\n",
    "TW2 = torch.from_numpy(W2)\n",
    "TW2.requires_grad=True\n",
    "tx = torch.from_numpy(x)\n",
    "ty = torch.from_numpy(y)\n",
    "# tnnx = tsigmoid(tx @ TW1) @ TW2 in smalller steps\n",
    "hidden_in = tx @ TW1\n",
    "hidden_out = tsigmoid(hidden_in)\n",
    "tnnx = hidden_out @ TW2\n",
    "hidden_in.retain_grad()\n",
    "hidden_out.retain_grad()\n",
    "tnnx.retain_grad()\n",
    "loss = (tnnx - ty)**2\n",
    "print('loss(x, y)', loss)\n",
    "print('nn(x): ',tnnx)\n",
    "# tnnx.backward(retain_graph=True)\n",
    "# print('nnx/TW2 grad\\n', TW2.grad)\n",
    "# print('nnx/TW1 grad\\n', TW1.grad)\n",
    "# TW1.grad.data.zero_()\n",
    "# TW2.grad.data.zero_()\n",
    "# tnnx.zero_grad()\n",
    "loss.backward()\n",
    "print('Results')\n",
    "print('L/TW2 grad\\n', TW2.grad)\n",
    "print('L/TW1 grad\\n', TW1.grad)\n",
    "print('Lets do the steps')\n",
    "print('L/nnx grad\\n', tnnx.grad)\n",
    "print('L/hidden_out grad\\n', hidden_out.grad)\n",
    "print('L/hidden_in grad\\n', hidden_in.grad)\n",
    "\n",
    "# tnnx.zero_grad()\n",
    "\n",
    "print('Lets do in steps we know the result')\n",
    "print('nnx/hidden_out grad\\n', hidden_out.grad)\n",
    "print('hidden_out/hidden_in grad\\n', TW2.grad)\n",
    "print('hidden_in/TW2 grad\\n', TW2.grad)\n",
    "print('nnx/TW2 grad\\n', TW2.grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
