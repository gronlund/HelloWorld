{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Gradient Calculation checked with pytorch\n",
    "First numpy program to evaluate the program.\n",
    "Then we switch to pytorch for the awesome automtic differentation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid(0): 0.5\n",
      "Forward pass:\n",
      " --------------------\n",
      "s: [0. 0.]\n",
      "a: [0.5 0.5]\n",
      "out: [2.]\n",
      "--------------------\n",
      "nn(x): [2.]\n",
      "Forward pass:\n",
      " --------------------\n",
      "s: [0. 0.]\n",
      "a: [0.5 0.5]\n",
      "out: [2.]\n",
      "--------------------\n",
      "loss((x,y)): [4.]\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([1.0, 2.0, 3.0, -4.0]).reshape(1,4)\n",
    "y = np.array([0.0]).reshape(1, 1)\n",
    "W1 = np.array([[2.0, 0.5, 3.0, 3.0], [-3.0, 3.0, -1.0, 0.0]]).T\n",
    "W2 = np.array([1.0, 3.0]).reshape(2, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1.0/(1+np.exp(x))\n",
    "\n",
    "def nn(x, W1, W2):\n",
    "    s = x@W1\n",
    "    a = sigmoid(s)\n",
    "    out = a @ W2\n",
    "    print('Forward pass:\\n','-'*20)\n",
    "    print('s:', s.ravel())\n",
    "    print('a:', a.ravel())\n",
    "    print('out:', out.ravel())\n",
    "    print('-'*20)\n",
    "    return out\n",
    "\n",
    "def loss(x, y, W1, W2):\n",
    "    pred = nn(x, W1, W2)\n",
    "    diff = pred - y\n",
    "    loss = diff**2\n",
    "    return loss\n",
    "print('sigmoid(0):', sigmoid(0))\n",
    "print('nn(x):', nn(x, W1, W2).ravel())\n",
    "print('loss((x,y)):', loss(x, y, W1, W2).ravel())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient of f(z)=1/(1+e^{-z}) at x = 0 tensor([ 0.2500])\n",
      "loss(x, y) tensor([[ 4.]], dtype=torch.float64)\n",
      "nn(x):  tensor([[ 2.]], dtype=torch.float64)\n",
      "Results\n",
      "L/TW2 grad\n",
      " tensor([[ 2.],\n",
      "        [ 2.]], dtype=torch.float64)\n",
      "L/TW1 grad\n",
      " tensor([[  1.,   3.],\n",
      "        [  2.,   6.],\n",
      "        [  3.,   9.],\n",
      "        [ -4., -12.]], dtype=torch.float64)\n",
      "Lets do the steps\n",
      "L/nnx grad\n",
      " tensor([[ 4.]], dtype=torch.float64)\n",
      "L/hidden_out grad\n",
      " tensor([[  4.,  12.]], dtype=torch.float64)\n",
      "L/hidden_in grad\n",
      " tensor([[ 1.,  3.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "# Automatic differentation\n",
    "import torch\n",
    "z = torch.zeros(1, requires_grad=True)\n",
    "sz = 1.0 / (1 + torch.exp(-z))\n",
    "sz.backward()\n",
    "print('gradient of f(z)=1/(1+e^{-z}) at x = 0', z.grad)\n",
    "\n",
    "def tsigmoid(z):    \n",
    "    return 1.0/(1.0 + torch.exp(-z))\n",
    "\n",
    "TW1 = torch.from_numpy(W1)\n",
    "TW1.requires_grad=True\n",
    "TW2 = torch.from_numpy(W2)\n",
    "TW2.requires_grad=True\n",
    "tx = torch.from_numpy(x)\n",
    "ty = torch.from_numpy(y)\n",
    "# tnnx = tsigmoid(tx @ TW1) @ TW2 in smalller steps\n",
    "hidden_in = tx @ TW1\n",
    "hidden_out = tsigmoid(hidden_in)\n",
    "tnnx = hidden_out @ TW2\n",
    "d = tnnx - ty\n",
    "s = d**2\n",
    "\n",
    "hidden_in.retain_grad()\n",
    "hidden_out.retain_grad()\n",
    "tnnx.retain_grad()\n",
    "d.retain_grad()\n",
    "print('loss(x, y)', s)\n",
    "print('nn(x): ',tnnx)\n",
    "# tnnx.backward(retain_graph=True)\n",
    "# print('nnx/TW2 grad\\n', TW2.grad)\n",
    "# print('nnx/TW1 grad\\n', TW1.grad)\n",
    "# TW1.grad.data.zero_()\n",
    "# TW2.grad.data.zero_()\n",
    "# tnnx.zero_grad()\n",
    "s.backward()\n",
    "print('Results')\n",
    "print('L/TW2 grad\\n', TW2.grad)\n",
    "print('L/TW1 grad\\n', TW1.grad)\n",
    "print('Lets do the steps')\n",
    "print('L/nnx grad\\n', tnnx.grad)\n",
    "print('L/hidden_out grad\\n', hidden_out.grad)\n",
    "print('L/hidden_in grad\\n', hidden_in.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
